\documentclass[sigconf]{acmart}

\input{format/final}
\graphicspath{ {./images/}}
\usepackage{subcaption}
% \usepackage[table]{xcolor}
\usepackage{url}

\begin{document}
\title{Analyzing everyday challenges of people with visual impairments}


\author{Tousif Ahmed}
\orcid{HID237}
\affiliation{%
\institution{Indiana University}
  \streetaddress{150 S Woodlawn Avenue}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47405}
}
\email{touahmed@indiana.edu}



\begin{abstract}
       People with visual impairments face varieties of problem in their daily lives. Nowadays, modern technology especially camera-based technologies are helping people with visual impairment in their everyday tasks ranging from daily household activity to navigation. Users are using camera based applications where they are sharing photos and asking questions. Based on the asked question and shared photo, automated tools or human crowd workers are helping the visually impaired people in their tasks. By exploring the questions, it is possible to understand the problems and challenges of people with visual impairments. However, the volume of such data makes it impossible to analyze the questions manually.  Big data analytics could help us to understand the challenges of people with visual impairments. To understand the challenges, we analyzed the VizWiz data set which contains more than 33,500 questions asked by people with visual impairments. In this paper, we report on the data and shed light on the challenges.  

\end{abstract}

\keywords{E534, HID 237,  Big Data, Accessibility Issues, People with Visual Impairments}


\maketitle


\section{Introduction}
People with visual impairments face a variety of problems in their daily lives and need assistance. They need assistance with detecting objects, identifying money, navigation, transportation, household activities, cooking, and various other activities. Sighted person on rely on vision on so many things that it is almost impossible to visualize and understand the problems of people with visual impairments. Although there are variety of tools available to simulate the challenges and experiences of people with visual impairments, the challenges of people with visual impairments is not well understood. To help visually impaired people with technologies, we need to understand their problem first.

One possible to understand the challenges of people with visual impairments is qualitative analysis or ethnographic studies with people with visual impairments. Simply, researchers can follow or conduct interviews with people with visual impairments. Although qualitative studies are widely accepted research methods, it has limitations. Specially to understand the problems of people with visual impairments, qualitative studies have severe limitations. As the challenges vary with the experiences of people with visual impairments, these studies can not capture or depict the whole picture. Besides, these studies are very expensive and need ample human effort. Therefore, we need a better way to understand the challenges of people with visual impairments.

Big data analytics could be a potential alternative. To understand how big data can help people with visual impairments, we need to understand the backgroungd first. Nowadays, people with visual impairments uses different technologies for their problems.A wide range of technologies such as talking watch\footnote{https://www.maxiaids.com/talking-watches}, braille reader\footnote{\url{http://www.afb.org/prodBrowseCatResults.aspx?CatID=43}}, navigation helper \footnote{\url{http://www.gdp-research.com.au/}} are available in the market to help the visually impaired in their daily tasks. Since the introduction of smartphone, smartphone based applications gained huge popularities among people with visual impairments. Now, mobile and smartphone applications like Seeing AI~\cite{seeingai}, AiPoly~\cite{aipoly},LookTel~\cite{looktel}, and other such camera based applications are helping people with visual impairmentsin object recognition, face recognition, color detection, human emotion detection, activity recognition, and other such tasks that was not possible before. Figure ~\ref{fig:seeingai} depicts an example from Seeing AI which shows that how camera based applications are helping people with visual impairments by describing nearby person's activitty (Figure ~\ref{fig:ios}) and their facial emotions (Figure ~\ref{fig:facial}) .
\begin{figure}[tbp]
        \centering
        \begin{subfigure}[b]{0.4\columnwidth}
                %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
                \includegraphics[width=\textwidth]{images/facial.pdf}  
                \caption{Age, gender, appearance, and facial expression} 
                  \label{fig:facial}   
        \end{subfigure}%
        ~ 
        \begin{subfigure}[b]{0.4\columnwidth}
                \includegraphics[width=\textwidth]{images/activity2.pdf}  
                 \caption{Age, gender, and activity of a nearby person}
                  \label{fig:ios}      
        \end{subfigure}%
        \caption{Seeing AI providing various information about people nearby~\cite{seeingai}.} 
        %Android currently displays the requested permissions during the app installation process. iOS  allows selective disabling of permissions for installed apps.}
        \label{fig:seeingai}
\end{figure}

Most of the camera assisted assitive applications works in one simple way.The user uploads an intended photo and asks a question about that. Applications have it's simple iq engine which tries to answer the problem first. If it's not able to answer that question, it shares the questions and images with the user's friends and family. Sometimes, the image is shared with a web based human worker. This crowd wroker is essential for such system, because the iq engine is not sophisticated yet. We can not completely trust the automated approaches.  Besides, visually impaired user's can not efficiently take photos. Sometimes they point at totally wrong objetcts or items, sometimes they share blurry photos, and even sometimes the question does not match with the photos~\cite{Jayant:2011,Bigham:2010,Harada:2013}. Therefore, to give a correct answer of the questions, technologies require human intelligence. Questions and answer based applications like TapTapSee~\cite{taptapsee} and VizWiz~\cite{Bigham:2010} uses this approach. LookTell~\cite{looktel} and BeMyEyes does not have any automated approach, it directly broadcasts the video feed to the volunteers and volunteers answer their questions. Some applications are trying to move towards the fully automated approach, however, due to the limitations of automated approaches they did not gain much popularity yet. 

Human based systems have privacy issues, because these users are uploading their photos which may contain sensitive information. Often they ask about medical information, their address, and various other sensitive information which can be exploited by the malicious crowd workers. Even sometimes, the users shares their credit card image and asks the system to about their credit card information which have severe privacy and security implications. Moreover, cameras and images shared by them can be extremely risky for people with visual impairments, becuase often they do not know the contents of the photo. Photos can be uploaded in error, sensitive data can be shared unintentionally. Ahmed et al.~\cite{Ahmed:2016} reported a scary story of one of the VizWiz users, who accidentally shared her naked photo with a crowd worker. Such evidents suggestsb that such systems has severe privacy and security implications. However, visually impaired needs such tool in their daily lives. Therefore, the ideal solution would be an completely automated approach. However, to design a flawless system we need to improve the existing tools first and we need to understand the challenges first.

The challenges of people with visual impairments can be easily understood from the images uploaded and questions asked by the user. Although it is extremely difficult for people with visual impairments to take a good photo, still they are using these tools because of their challenges. Therefore, the data uploaded in these applications are probably a good way to understand the challenges. However, due to the volume of the data it is not possible to manually identify the problems. Therefore, big data analytics can be helpful in this context to understand the challenges of people with visual impairments. However, due to privacy issues all but one data sets are not publicly available. 

In this paper, we analyzed the VizWiz data set containing more than 35000 images and questions~\cite{Bigham:2010}. Based on the questions asked, we tried to categorize their problems which eventually help the researchers to design and develop a fully automated system.Previous researches~\cite{Brady:2013} explored the same problem with the same data set. However, they only explored 1000 images and performed a qualitative study and identified four categories of problem. Since manual analysis is not possible on 35000 data, we used big data tools to automatically analyze the questions and images. In this paper, we report on analysis performed and the visual challenges of people with visual impairments.

\section{Methodology}
In this section, we discuss the methodology for identifying the challenges of people with visual impairments. 

\subsection{Data Set}
We used VizWiz data set which is the only available data set in this category. VizWiz is an iPhone application that allows visually impaired users to get quick responses of their challenges. The app tries to find an answer by using automatic IQ engine and anonymous crowd workers~\cite{vizwiz}. VizWiz was released in May, 2011. 

VizWiz application helped more than ten thousand users by answering more than 100,000 questions. However, they only shared half of their data from those participants who gave consent. Therefore, around 50,000 data are available for the researchers. However, the researchers removed around 6,000 data due to sensitive contents in the images. The rest of the 43,543 images were made public. All images and questions redundantly checked and anonymized. We downloaded this data set for research purposes in May. Recently, the data is not available to download.Therefore, we urge the instructors to not distribute the data. 

Based on the images shared and questions uploaded, we found only 33,580 images and their related questions. The questions were shared in json format and images are shared in a compressed directory. The questions data set have three columns which I described next:
\begin{itemize}
    \item \textbf{image}: The name of the image file. 
    \item \textbf{private}: If the image is marked as private.
    \item \textbf{question}: Asked questions of the user. Some questions are missing. 
    \item \textbf{response}: Each question can have multiple responses. As mentioned earlier, some questions were tried to answer using the IQ engine and some questions were sent to the web workers. For each question, there can be one to 11 responses. However, on average three responses were received. The distribution of the responses shown in Figure~\ref{f:response count}. From  the figure, we can see that most of the questions either received three or four responses.
    \begin{figure}[!ht]
 \centering\includegraphics[width=\columnwidth]{images/r_count.png}
  \caption{Distribution of the number of responses}
  \label{f:response count}
\end{figure}

\end{itemize}

\subsection{Data Cleaning and Preparation}
Since this data was collected from a research group, the data is very clean. We did not need any further cleaning except we discarded the private column. Since the researchers did not share the private images, therefore, all the rows in private columns shows false. Therefore, this column does not add any values to our analysis. We also noticed that lot of questions are missing, but an image is available. We can safely assume that these images were asked without questions and the users assumed that the images are self describing. Since the images can be interesting, therefore, we still kept the questions and labeled those questions as `NoQues'.We used `pandas' for storing the data. We also uploaded the images in the specified Google Drive folder.

\subsection{Data Analysis}
We performed analysis on both the questions and image data sets. The image data set only used to detect the blurred images. However, we rigorously analyzed the questions to identify various issues of people with visual impairments. In this section, we mainly discussed the text analysis methods. The full analysis can be found in 'question\_analysis' jupyter notebook. The image analysis can be found in 'image\_analysis' notebook. 

\subsubsection{Question Analysis}
To understand the challenges of people with visual impairments we performed unigram, bigram, and trigram analysis. Based on the analysis, we identified several issues which is presented in the results. The process of identifying the challenges is discussed in next section.



\section{Results}
In this section,we present our results that we identified from the analysis:
\subsection{Identifying the challenges of people with visual impairments}
The questions asked by people with visual impairments explains some of their challenges in their daily lives. Whenever they are facing issues, they are asking questions in VizWiz. Therefore, the questions asked could give us some insights about their challenges.
\begin{figure}[hbp]
        \centering
        
        %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
        \includegraphics[width=\columnwidth]{images/uni_all.png}  
        \caption{Most frequently used words in the questions asked} 
          \label{fig:uni_all}   
      
        
\end{figure}
To understand the challenges, we first calculated the frequency of the words. There are around 4500 unique words in the questions. The most frequent 50 words is shown in Figure~\ref{fig:uni_all}.  If we closely examine the words, we can see that the most frequently used word is \emph{`what'}. \emph{`What'} appeared 22793 times which is approximately 70\% of all of the worlds. The second and third most frequent words are `this' and `the'. Since, this is a set of questions, therefore, all the above words are justifiable.  Although, 1what' is somewhat giving us an indication that users are asking about objects or subjective questions mostly, 'this and 'the' is not adding that much value.  Next, we performed the same analysis by removing the most commonly used words in English. That unigrams gave us some additional insights. The list of most freuquntly used interesting words can be found in Figure ~\ref{fig:uni_int}.  If we remove the commonly used words, then for the majority of the questions had no questions. Those questions were asked by just uploading the photos.  We assume that the users thought that the app could automatically answer those questions. Other three most frequently used words are 'color', 'tell', and 'please'. Among these three the most interesting is `color'. Combination of `what' and `color' indicates that people with visual impairments faces issues with color detection, and often they ask the workers about the color of the objects and items.  Therefore, we found \textbf{color detection} problem of people with visual impairments from the analysis.  If we just consider the nouns and pronouns from the 30 most frequently used words, we find `box', `picture', `color', `screen', `shirt', `bottle', `flavor', `brand', `coffee', `label', and `product'. From this keywords, we can safely assume three other problems: they face issues with screens (screen), there are issues with objects (brand), and the users face issues with reading labels. Therefore, from the initial analysis we found four problems that people with visual impairments regularly face: \textbf{color detection}, \textbf{object detection}, \textbf{reading screens (mobile/ computer)}, and \textbf{reading labels}. 

\begin{figure}[hbp]
        \centering
        
        %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
        \includegraphics[width=\columnwidth]{images/int_count.png}  
        \caption{Most frequently used interesting words} 
          \label{fig:uni_int}   
      
        
\end{figure}

After checking the most frequently used words, we explored the most interesting pairs of words. If we check the bigrams (Figure ~\ref{fig:bi_all} and ~\ref{fig:bi_int} in Appendix), it gives confidence of our identified problems. The most frequently used two words are `what' and `this' which suggests that most questions were asked to identify the object. Therefore, people with visual impairments definitely face problems with detecting objects. `What' and `color' also suggests that users face color detection problem frequently. If we check the bigrams of most frequently used interesting words (Figure ~\ref{fig:bi_int}), we find some additional insights. If we ignore `NoQues', then we again see color detection and computer screen reading problem. However, now we can find another interesting pairs of words `look' and `like'. This pair indicates a subjective question, where the user is asking how the user is looking like. This identifies another challenges of people with visual impairments \textbf{Impression Management}. Another interesting common pair of words are 'long' and 'cook' which indicates reading label issues, however this can be a household activity issue. The trigrams also gave us some new interesting insights (Figure ~\ref{fig:tri_int}). Most of the trigrams confirms above mentioned challenges, however, there are some new issues. One interesting trigram is `display', `treadmill', and `tell' which indicates the health fitness related issues or \textbf{Health Management} Issue. Due to the accessibility issues in health monitoring and fitness monitoring issues, they can not manage health effective. Therefore, the users often seek help for reading the display. Another interesting three words are `pregnancy',`test',`show' which can also be put into health Management category. However, this seems a private information, but still people with visual impairments have to share this information due to their visual challenges.


\subsection{Challenges}
Based on the rigorous analysis, we identified some challenges of people with visual impairments. In this section, we discuss the challenges:

\subsubsection{Object Detection:} The most frequently asked question in VizWiz is `What is this' or `What is that'. `What' appeared more than 22,000 questions. Among those 22,000 questions around 7,000 questions are `What is this?' and `What is that?'. People asks variety of object detection questions ranging from everyday objects to personal objects. Some examples of  object detection problem is shown in Figure ~\ref{fig:object}.By manually analyzing some photos, it seems most of them are related to household activities. Therefore,with better tools it is possible to detect the objects. 
\begin{figure}[hbp]
        \centering
        \begin{subfigure}[b]{0.4\columnwidth}
                %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
                \includegraphics[width=\textwidth]{images/object_1.pdf}  
        \end{subfigure}%
        ~ 
        \begin{subfigure}[b]{0.4\columnwidth}
                \includegraphics[width=\textwidth]{images/object_2.pdf}  
        \end{subfigure}%
        \caption{Object Detection Questions} 
        %Android currently displays the requested permissions during the app installation process. iOS  allows selective disabling of permissions for installed apps.}
        \label{fig:object}
\end{figure}



\subsubsection{Color Detection:} Another most frequent problem that people with visual impairments face is to detect colors. Most of the time they use VizWiz to identify colors of their cloths, items, foods,and others. Some examples of color detection is shown in Figure~\ref{fig:color}.  Based on the images, automatically detecting the colors seems a challenging task. Because, if we examine figure ~\ref{fig:color} we can see in the image there can be other objects. Automatically detecting the object of interest will be difficult. For example, in the right most photo the user is asking about the color of the dress in hand, however, there are other objects  visible in the photo. Therefore, identifying the color automatically will be challenging. 
\begin{figure}[hbp]
        \centering
        \begin{subfigure}[b]{0.3\columnwidth}
                %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
                \includegraphics[width=\textwidth]{images/color_1.pdf}  
        \end{subfigure}%
        ~ 
        \begin{subfigure}[b]{0.3\columnwidth}
                \includegraphics[width=\textwidth]{images/color_2.pdf}  
        \end{subfigure}%
        \begin{subfigure}[b]{0.3\columnwidth}
                \includegraphics[width=\textwidth]{images/color_3.pdf}  
        \end{subfigure}%
        \caption{Color Detection images} 
        %Android currently displays the requested permissions during the app installation process. iOS  allows selective disabling of permissions for installed apps.}
        \label{fig:color}
\end{figure}

\subsubsection{Reading Screens:} Nowadays, people with visual impairments use smartphones and computers. They use screen reading software which generates synthesized speech to relay the information from screen. However, some times these software fail and visually impaired need to seek help from crowd workers. Another issue is the accessibility issues of CAPTCHA, people with visual impairments struggles with CAPTCHA. Therefore, they seek people who can read the CAPTCHA for them. Some examples of reading screen problems are shown in Figure~\ref{fig:screen}. 
\begin{figure}[hbp]
        \centering
        \begin{subfigure}[b]{0.45\columnwidth}
                %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
                \includegraphics[width=\textwidth]{images/screen_1.pdf}  
        \end{subfigure}%
        ~ 
        \begin{subfigure}[b]{0.45\columnwidth}
                \includegraphics[width=\textwidth]{images/screen_2.pdf}  
        \end{subfigure}%
       
        \caption{Screen Reading images} 
        %Android currently displays the requested permissions during the app installation process. iOS  allows selective disabling of permissions for installed apps.}
        \label{fig:screen}
\end{figure}

\subsubsection{Reading documents or labels:} Another obvious challenges of people with visual impairments is reading documents. The paper documents are not often accessible and people need help from others to read that. People might use scanners to read documents, however, scanning documents can be time consuming. Especially, for scanning food or medicine labels can be difficult. Therefore, participants seek help to read labels for them.Figure~\ref{fig:reading} shows some examples of reading issues. However, there can be potential score for technology for this types of problem. If the user is asking for reading helps, a simple OCR can help. However, OCR might not work well with food labels. One suggestion could be for food related reading question, the system could look for barcode and identify necessary information.
\begin{figure}[hbp]
        \centering
        \begin{subfigure}[b]{0.3\columnwidth}
                %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
                \includegraphics[width=\textwidth]{images/reading_1.pdf}  
        \end{subfigure}%
        ~ 
        \begin{subfigure}[b]{0.3\columnwidth}
                \includegraphics[width=\textwidth]{images/reading_2.pdf}  
        \end{subfigure}%
        \begin{subfigure}[b]{0.3\columnwidth}
                \includegraphics[width=\textwidth]{images/reading_3.pdf}  
        \end{subfigure}%
       
        \caption{Reading problems related images} 
        %Android currently displays the requested permissions during the app installation process. iOS  allows selective disabling of permissions for installed apps.}
        \label{fig:reading}
\end{figure}

\subsubsection{Impression Management:} Based on the analysis, we explored that managing impressions can be challenging. As a social norm, we often present our better selves to others by wearing consistent dresses. For example, we do not want to present ourselves in social places in such way that may misrepresent ourselves. Some words that we found in the questions are `look', `like' which we assume that users are asking to understand their appearance. Therefore, impression management for people could be challenging. Sometimes, the questions can be appearance related. Some examples of impression management challenges is shown in Figure ~\ref{fig:impression}.
\begin{figure}[hbp]
        \centering
        \begin{subfigure}[b]{0.45\columnwidth}
                %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
                \includegraphics[width=\textwidth]{images/impression_1.pdf}  
        \end{subfigure}%
        ~ 
        \begin{subfigure}[b]{0.45\columnwidth}
                \includegraphics[width=\textwidth]{images/impression_2.pdf}  
        \end{subfigure}%
       
        \caption{Questions asked containing `look' and `like'} 
        %Android currently displays the requested permissions during the app installation process. iOS  allows selective disabling of permissions for installed apps.}
        \label{fig:impression}
\end{figure}

\subsubsection{Health Management:} Health management is important for everyone. However, people with visual impairments face lot of challenges to maintain healthy behavior. They struggles to cook, therefore, they need to eat outside or eat packaged foods. They can not read the package's well, so miss the nutrition info. Managing medicine can be issue. Some other issues can be attributed to visual representation of results. For examples, weight scales show visual weights, pregnancy scales convey visual feedback, health monitoring instruments like treadmill convey visual information. All these visual information makes it difficult for managing health issues. Therefore, health management can be challenging. For that reasons, people with visual impairments often ask such applications to help them with various visual indicators in health and fitness. Figure ~\ref{fig:health} shows three different health realted issues of people with visual impairments. Figure~\ref{fig:pill} depicts the issues of medicine management, users often can not identify the required medicine. Figure ~\ref{fig:preg} shows asking the result of pregnancy test, which can be sensitive. Figure~\ref{fig:weight} asking questions about the weight of the user. Since, such applications can forward these questions to friends and family members all these images can be sensitive. However, technology can potentially address this issue by automating the responses.
\begin{figure}[hbp]
        \centering
        \begin{subfigure}[b]{0.3\columnwidth}
                %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
                \includegraphics[width=\textwidth]{images/health_1.pdf}  
                \caption{Question asking pill information}
                \label{fig:pill}
        \end{subfigure}%
        ~ 
        \begin{subfigure}[b]{0.3\columnwidth}
                \includegraphics[width=\textwidth]{images/health_2.pdf}
                \caption{Question asking the result of pregnancy test}
                \label{fig:preg}
        \end{subfigure}%
        \begin{subfigure}[b]{0.3\columnwidth}
                \includegraphics[width=\textwidth]{images/health_3.pdf} 
                 \caption{Question asking the weight}
                 \label{fig:weight}
        \end{subfigure}%
        \caption{Various health related questions} 
        %Android currently displays the requested permissions during the app installation process. iOS  allows selective disabling of permissions for installed apps.}
        \label{fig:health}
\end{figure}

\subsubsection{Taking Photos:} Like sighted people, visually impaired people also wants to take photos. However, taking photos are challenging since the users can not seen the image. Therefore, they often struggle to take photos. The irony of applications like VizWiz is that these services require a challenging task to solve other challenges. Although none of the questions mention anything about taking photos, the responses of the web workers illustrates the photo taking challenges of people with visual impairments. Around 4000 images have been detected as blurry and not understandable by human workers. Apart from blurry images, sometimes photos can be out of focus and misplaced. 

\begin{figure}[hbp]
        \centering
        \begin{subfigure}[b]{0.3\columnwidth}
                %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
                \includegraphics[width=\textwidth]{images/blurry.jpg}  
                \caption{Blurry Photo}
                \label{fig:blur}
        \end{subfigure}%
        ~ 
        \begin{subfigure}[b]{0.3\columnwidth}
                \includegraphics[width=\textwidth]{images/poor_f.pdf}
                \caption{Poorly Focused photo}
                \label{fig:pf}
        \end{subfigure}%
        \begin{subfigure}[b]{0.3\columnwidth}
                \includegraphics[width=\textwidth]{images/off_focus.pdf} 
                 \caption{Off Focused photo}
                 \label{fig:off}
        \end{subfigure}%
        \caption{Poorly captured images} 
        %Android currently displays the requested permissions during the app installation process. iOS  allows selective disabling of permissions for installed apps.}
        \label{fig:photo}
\end{figure}
Figure ~\ref{fig:photo} depicts the some not understandable photos taken by people with visual impairments. However, such images takes resources and often cost money. If the system can early detect such images and prevent those images from sending then it can save resources. Misplaced or blurry photos can be early detected. Another potential scope of technology is to automatically fix the blurry images. 


We identified various challenges of people with visual impairments. There can be other challenges, however, from the VizWiz data set these seven seems some major problems. We also discovered that there can be privacy issues with the shared images (i.e., pregnancy test results) and such data need to be handled carefully. Although existing services require manual efforts, technology has various scopes to help people with visual impairments. Due to poor quality of images, such system may consume significant user resources and early detection of the quality of images can save the resources. In the next section, we discuss one such approach and the evaluation of the approach using VizWiz data set. 

\subsection{Automatically Detecting Blurry Images}
We have already give some examples of the struggles of taking photos by the user. Often their photos are out of focus and blurry. Using OpenCV, we can detect blurry images. From the web workers responses we have an estimation that some photos are very blurry and can not be recognizable by human. If the system can early detect the blurry photos and asks the user to retake the photos it could reduce human effort. In this analysis, our task is if we can automatically identify the blurred images. The `Image Analysis' jupyter notebook shows some the analysis that we performed in this section.

\subsubsection{Estimation of Ground Truth Data}
 We set up the ground truth from the web workers responses. If any of the web workers mentioned that the image is blurry, then we set the image as blurry. From that, we found a list of 3580 images which can be considered as blurry. We then divided the data frame into two different sets: blurred set and not blurred set. 

\subsubsection{Detecting Blurry Photos} We followed pyimagesearch's tutorial to detect the blur images~\cite{blur}. Following that tutorial, we used variance of the Laplacian to detect the blurred images. Then, we run  the algorithm on 33,580 images. 

\subsubsection{Calculating $F1$ score} We made an assumption for the accuracy of blur detection. If we consider the real case scenario, if the user need to take a photo more than once to avoid blurring that is not a problem. Although, they have difficulties of taking photos but it is still possible to take a better photo and there is no cost of taking photos. However, if we send a blurry photo to web worker it wastes resources. The system need to pay the web workers for their tasks and the system somehow charges that money to the users. Therefore, taking a blurry photo is costly. Therefore, for such a system it is better to be some false positives than false negatives. Therefore, this system tries to reduce the false negatives. Hence, we tried to improve the recall. However, too much false positive can affect the usability of a system. Therefore,Our target is to find the best accuracy over blurred images minimizing false positive rates. F1 score will help us to find a correct threshold. Our initial threshold of 150 gave us F1 score of 21.36\%.

\subsubsection{Identifying a good threshold} We run the algorithm with various thresholds. The F1 score graph against various thresholds did not improve the accuracy. Figure ~\ref{fig:accuracy} shows the accuracy of blur detection.
\begin{figure}[hbp]
        \centering
        
        %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
        \includegraphics[width=\columnwidth]{images/f1.png}  
        \caption{Accuracy of Laplacian blur detection } 
          \label{fig:accuracy}   
      
        
\end{figure}
\subsubsection{Implications of result} The poor accuracy of the blur detection algorithms depicts some problems of real world data set. Although Laplacian blur detection is a good indicator is a good indicator of blurred images, the algorithm failed in this case. The failure of the blur detection algorithm can be attributed to poorly taken images and inconsistent image sizes. We tried to change the size of the images, however, it did not improve the accuracy of the results. Probably using new deep learning based methods will be more effective. 


\subsection{Privacy implications of VizWiz}
In the analysis of VizWiz, we have seen various issues of people with visual impairment. Definitely, such applications are helping the users, making them more independent. However, there are privacy risks. We have seen people share their medical health information, often they share their address web workers. The authors of VizWiz data set did not share 5000 photos due to privacy reason. However, people often share their credit card information which can have severe consequences. The information given to unfamiliar people can be exploited. Therefore, additional care is required for such data. Based on the analysis, we have seen multiple times that it is not always possible to automatically answering the questions. We need human intelligence for some challenges. If the data requires human intelligence, then instead of sending the complete data the system can send partial data so that the privacy implication can be reduced.

Another potential privacy threat can be arose from the inability to know what is in the picture. The user can mistakenly capture sensitive photos and share it with the web workers. The bystanders of such devices are also in risk, because they can also inadvertently captured by the user and shared with the crowd workers.One such example is shown in Figure~\ref{fig:privacy}. If we check the figure, we can see that a bystander is present in the picture. The question asked for this question was `What is this?'. We can assume that the user probably was trying to detect an object but took a photo of nearby person. Similar privacy leakage can happen with credit cards, and other sensitive information. Photos can be shared in error. Therefore, such systems should consider such implications and should take extra precaution to reduce such incidents. 
\begin{figure}[tbp]
        \centering
        
        %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
        \includegraphics[width=\columnwidth]{images/private_1.pdf}  
        \caption{Privacy Implications of VizWiz} 
          \label{fig:privacy}   
      
        
\end{figure}

\section{Conclusion}
People with visual impairments faces different challenges and by analyzing the VizWiz data set we identified and explored some challenges. Although some challenges could be identified by analyzing portions of the data, big data analytics helps us to get a better exploration of the challenges. Moreover, big data analytics also helps to discover some solution space. In future, if other such services similar analysis it would be possible to reduce the human effort that is required to operate such services. Moreover, with more data it would be possible to early detect the risks. By early detecting the risks, the system would be more helpful for people with visual impairments. Only in that way, they can enjoy the similar quality like other sighted people. 
\begin{acks}

The authors would like to thank Professor Gregor von Laszewski for helping us with the instruction and resources that were required to complete this paper. We would also to like to thank the associate instructors for being available on the course website all the time and helping us with their answers.

\end{acks}



\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 
\newpage
\appendix
\section{Appendix}
\noindent\begin{minipage}{\textwidth}
    \centering
    \captionof{figure}{Most frequently used pair of word}
    \includegraphics[scale=0.45]{images/bigram_all.png}
     \label{fig:bi_all}   
\end{minipage}
\noindent\begin{minipage}{\textwidth}
    \centering
    \captionof{figure}{Most frequently used pair of interesting words}
    \includegraphics[scale=0.45]{images/bigram_interesting.png}
     \label{fig:bi_int}   
\end{minipage}
%The bigram and trigram images. 


\begin{figure*}[bp]
        \centering
        %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
        \includegraphics[scale=0.5]{project/images/trigram_interesting.png}  
        \caption{Most frequently used interesting words} 
          \label{fig:tri_int}   
\end{figure*}

%We include an appendix with common issues that we see when students submit papers. One particular important issue is not to use the underscore in bibtex labels. Sharelatex allows this, but the proceedings script we have does not allow this.

%When you submit the paper you need to address each of the items in the
%issues.tex file and verify that you have done them. Please do this
%only at the end once you have finished writing the paper. To d this
%cange TODO with DONE. However if you check something on with DONE, but
%we find you actually have not executed it correcty, you will receive
%point deductions. Thus it is important to do this correctly and not
%just 5 minutes before the deadline. It is better to do a late
%submission than doing the check in haste. 

%\input{issues}

\end{document}
