\documentclass[sigconf]{acmart}

\input{format/final}
\graphicspath{ {./images/}}
\usepackage{subcaption}
% \usepackage[table]{xcolor}
\usepackage{url}

\begin{document}
\title{Analyzing everyday challenges of people with visual impairments}


\author{Tousif Ahmed}
\orcid{HID237}
\affiliation{%
\institution{Indiana University}
  \streetaddress{150 S Woodlawn Avenue}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47405}
}
\email{touahmed@indiana.edu}



\begin{abstract}
       People with visual impairments face varieties of problem in their daily lives. Nowadays, modern technology especially camera-based technologies are helping people with visual impairment in their everyday tasks ranging from daily household activity to navigation. Users are using camera based applications where they are sharing photos and asking questions. Based on the asked question and shared photo, automated tools or human crowd workers are helping the visually impaired people in their tasks. By exploring the questions, it is possible to understand the problems and challenges of people with visual impairments. However, the volume of such data makes it impossible to analyze the questions manually.  Big data analytics could help us to understand the challenges of people with visual impairments. To understand the challenges, we analyzed the VizWiz data set which contains more than 33,500 questions asked by people with visual impairments. In this paper, we report on the data and shed light on the challenges.  

\end{abstract}

\keywords{E534, HID 237,  Big Data, Accessibility Issues, People with Visual Impairments}


\maketitle


\section{Introduction}
People with visual impairments face a variety of problems in their daily lives and need assistance. They need assistance with detecting objects, identifying money, navigation, transportation, household activities, cooking, and various other activities. Sighted person on rely on vision on so many things that it is almost impossible to visualize and understand the problems of people with visual impairments. Although there are variety of tools available to simulate the challenges and experiences of people with visual impairments, the challenges of people with visual impairments is not well understood. To help visually impaired people with technologies, we need to understand their problem first.

One possible to understand the challenges of people with visual impairments is qualitative analysis or ethnographic studies with people with visual impairments. Simply, researchers can follow or conduct interviews with people with visual impairments. Although qualitative studies are widely accepted research methods, it has limitations. Specially to understand the problems of people with visual impairments, qualitative studies have severe limitations. As the challenges vary with the experiences of people with visual impairments, these studies can not capture or depict the whole picture. Besides, these studies are very expensive and need ample human effort. Therefore, we need a better way to understand the challenges of people with visual impairments.

Big data analytics could be a potential alternative. To understand how big data can help people with visual impairments, we need to understand the backgroungd first. Nowadays, people with visual impairments uses different technologies for their problems.A wide range of technologies such as talking watch\footnote{https://www.maxiaids.com/talking-watches}, braille reader\footnote{\url{http://www.afb.org/prodBrowseCatResults.aspx?CatID=43}}, navigation helper \footnote{\url{http://www.gdp-research.com.au/}} are available in the market to help the visually impaired in their daily tasks. Since the introduction of smartphone, smartphone based applications gained huge popularities among people with visual impairments. Now, mobile and smartphone applications like Seeing AI~\cite{seeingai}, AiPoly~\cite{aipoly},LookTel~\cite{looktel}, and other such camera based applications are helping people with visual impairmentsin object recognition, face recognition, color detection, human emotion detection, activity recognition, and other such tasks that was not possible before. Figure ~\ref{fig:seeingai} depicts an example from Seeing AI which shows that how camera based applications are helping people with visual impairments by describing nearby person's activitty (Figure ~\ref{fig:ios}) and their facial emotions (Figure ~\ref{fig:facial}) .
\begin{figure}[tbp]
        \centering
        \begin{subfigure}[b]{0.4\columnwidth}
                %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
                \includegraphics[width=\textwidth]{images/facial.pdf}  
                \caption{Age, gender, appearance, and facial expression} 
                  \label{fig:facial}   
        \end{subfigure}%
        ~ 
        \begin{subfigure}[b]{0.4\columnwidth}
                \includegraphics[width=\textwidth]{images/activity2.pdf}  
                 \caption{Age, gender, and activity of a nearby person}
                  \label{fig:ios}      
        \end{subfigure}%
        \caption{Seeing AI providing various information about people nearby~\cite{seeingai}.} 
        %Android currently displays the requested permissions during the app installation process. iOS  allows selective disabling of permissions for installed apps.}
        \label{fig:seeingai}
\end{figure}

Most of the camera assisted assitive applications works in one simple way.The user uploads an intended photo and asks a question about that. Applications have it's simple iq engine which tries to answer the problem first. If it's not able to answer that question, it shares the questions and images with the user's friends and family. Sometimes, the image is shared with a web based human worker. This crowd wroker is essential for such system, because the iq engine is not sophisticated yet. We can not completely trust the automated approaches.  Besides, visually impaired user's can not efficiently take photos. Sometimes they point at totally wrong objetcts or items, sometimes they share blurry photos, and even sometimes the question does not match with the photos~\cite{Jayant:2011,Bigham:2010,Harada:2013}. Therefore, to give a correct answer of the questions, technologies require human intelligence. Questions and answer based applications like TapTapSee~\cite{taptapsee} and VizWiz~\cite{Bigham:2010} uses this approach. LookTell~\cite{looktel} and BeMyEyes does not have any automated approach, it directly broadcasts the video feed to the volunteers and volunteers answer their questions. Some applications are trying to move towards the fully automated approach, however, due to the limitations of automated approaches they did not gain much popularity yet. 

Human based systems have privacy issues, because these users are uploading their photos which may contain sensitive information. Often they ask about medical information, their address, and various other sensitive information which can be exploited by the malicious crowd workers. Even sometimes, the users shares their credit card image and asks the system to about their credit card information which have severe privacy and security implications. Moreover, cameras and images shared by them can be extremely risky for people with visual impairments, becuase often they do not know the contents of the photo. Photos can be uploaded in error, sensitive data can be shared unintentionally. Ahmed et al.~\cite{Ahmed:2016} reported a scary story of one of the VizWiz users, who accidentally shared her naked photo with a crowd worker. Such evidents suggestsb that such systems has severe privacy and security implications. However, visually impaired needs such tool in their daily lives. Therefore, the ideal solution would be an completely automated approach. However, to design a flawless system we need to improve the existing tools first and we need to understand the challenges first.

The challenges of people with visual impairments can be easily understood from the images uploaded and questions asked by the user. Although it is extremely difficult for people with visual impairments to take a good photo, still they are using these tools because of their challenges. Therefore, the data uploaded in these applications are probably a good way to understand the challenges. However, due to the volume of the data it is not possible to manually identify the problems. Therefore, big data analytics can be helpful in this context to understand the challenges of people with visual impairments. However, due to privacy issues all but one data sets are not publicly available. 

In this paper, we analyzed the VizWiz data set containing more than 35000 images and questions~\cite{Bigham:2010}. Based on the questions asked, we tried to categorize their problems which eventually help the researchers to design and develop a fully automated system.Previous researches~\cite{Brady:2013} explored the same problem with the same data set. However, they only explored 1000 images and performed a qualitative study and identified four categories of problem. Since manual analysis is not possible on 35000 data, we used big data tools to automatically analyze the questions and images. In this paper, we report on analysis performed and the visual challenges of people with visual impairments.

\section{Methodology}
In this section, we discuss the methodology for identifying the challenges of people with visual impairments. 

\subsection{Data Set}
We used VizWiz data set which is the only available data set in this category. VizWiz is an iPhone application that allows visually impaired users to get quick responses of their challenges. The app tries to find an answer by using automatic IQ engine and anonymous crowd workers~\cite{vizwiz}. VizWiz was released in May, 2011. 

VizWiz application helped more than ten thousand users by answering more than 100,000 questions. However, they only shared half of their data from those participants who gave consent. Therefore, around 50,000 data are available for the researchers. However, the researchers removed around 6,000 data due to sensitive contents in the images. The rest of the 43,543 images were made public. All images and questions redundantly checked and anonymized. We downloaded this data set for research purposes in May. Recently, the data is not available to download.Therefore, we urge the instructors to not distribute the data. 

Based on the images shared and questions uploaded, we found only 33,580 images and their related questions. The questions were shared in json format and images are shared in a compressed directory. The questions data set have three columns which I described next:
\begin{itemize}
    \item \textbf{image}: The name of the image file. 
    \item \textbf{private}: If the image is marked as private.
    \item \textbf{question}: Asked questions of the user. Some questions are missing. 
    \item \textbf{response}: Each question can have multiple responses. As mentioned earlier, some questions were tried to answer using the IQ engine and some questions were sent to the web workers. For each question, there can be one to 11 responses. However, on average three responses were received. The distribution of the responses shown in Figure~\ref{f:response count}. From  the figure, we can see that most of the questions either received three or four responses.
    \begin{figure}[!ht]
 \centering\includegraphics[width=\columnwidth]{images/r_count.png}
  \caption{Distribution of the number of responses}
  \label{f:response count}
\end{figure}

\end{itemize}

\subsection{Data Cleaning and Preparation}
Since this data was collected from a research group, the data is very clean. We did not need any further cleaning except we discarded the private column. Since the researchers did not share the private images, therefore, all the rows in private columns shows false. Therefore, this column does not add any values to our analysis. We also noticed that lot of questions are missing, but an image is available. We can safely assume that these images were asked without questions and the users assumed that the images are self describing. Since the images can be interesting, therefore, we still kept the questions and labeled those questions as `NoQues'.We used `pandas' for storing the data. We also uploaded the images in the specified Google Drive folder.

\subsection{Data Analysis}
We performed analysis on both the questions and image data sets. The image data set only used to detect the blurred images. However, we rigorously analyzed the questions to identify various issues of people with visual impairments. In this section, we mainly discussed the text analysis methods. The full analysis can be found in 'question\_analysis' jupyter notebook. The image analysis can be found in 'image\_analysis' notebook. 

\subsubsection{Question Analysis}
To understand the challenges of people with visual impairments we performed unigram, bigram, and trigram analysis. Based on the analysis, we identified several issues which is presented in the results. The process of identifying the challenges is discussed in next section.



\section{Results}
In this section,we present our results that we identified from the analysis:
\subsection{Identifying the challenges of people with visual impairments}
The questions asked by people with visual impairments explains some of their challenges in their daily lives. Whenever they are facing issues, they are asking questions in VizWiz. Therefore, the questions asked could give us some insights about their challenges.
\begin{figure}[hbp]
        \centering
        
        %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
        \includegraphics[width=\columnwidth]{images/uni_all.png}  
        \caption{Most frequently used words in the questions asked} 
          \label{fig:uni_all}   
      
        
\end{figure}
To understand the challenges, we first calculated the frequency of the words. There are around 4500 unique words in the questions. The most frequent 50 words is shown in Figure~\ref{fig:uni_all}.  If we closely examine the words, we can see that the most frequently used word is \emph{`what'}. \emph{`What'} appeared 22793 times which is approximately 70\% of all of the worlds. The second and third most frequent words are `this' and `the'. Since, this is a set of questions, therefore, all the above words are justifiable.  Although, 1what' is somewhat giving us an indication that users are asking about objects or subjective questions mostly, 'this and 'the' is not adding that much value.  Next, we performed the same analysis by removing the most commonly used words in English. That unigrams gave us some additional insights. The list of most freuquntly used interesting words can be found in Figure ~\ref{fig:uni_int}.  If we remove the commonly used words, then for the majority of the questions had no questions. Those questions were asked by just uploading the photos.  We assume that the users thought that the app could automatically answer those questions. Other three most frequently used words are 'color', 'tell', and 'please'. Among these three the most interesting is `color'. Combination of `what' and `color' indicates that people with visual impairments faces issues with color detection, and often they ask the workers about the color of the objects and items.  Therefore, we found \textbf{color detection} problem of people with visual impairments from the analysis.  If we just consider the nouns and pronouns from the 30 most frequently used words, we find `box', `picture', `color', `screen', `shirt', `bottle', `flavor', `brand', `coffee', `label', and `product'. From this keywords, we can safely assume three other problems: they face issues with screens (screen), there are issues with objects (brand), and the users face issues with reading labels. Therefore, from the initial analysis we found four problems that people with visual impairments regularly face: \textbf{color detection}, \textbf{object detection}, \textbf{reading screens (mobile/ computer)}, and \textbf{reading labels}. 

\begin{figure}[hbp]
        \centering
        
        %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
        \includegraphics[width=\columnwidth]{images/int_count.png}  
        \caption{Most frequently used interesting words} 
          \label{fig:uni_int}   
      
        
\end{figure}

After checking the most frequently used words, we explored the most interesting pairs of words. If we check the bigrams (Figure ~\ref{fig:bi_all} and ~\ref{fig:bi_int} in Appendix), it gives confidence of our identified problems. The most frequently used two words are `what' and `this' which suggests that most questions were asked to identify the object. Therefore, people with visual impairments definitely face problems with detecting objects. `What' and `color' also suggests that users face color detection problem frequently. If we check the bigrams of most frequently used interesting words (Figure ~\ref{fig:bi_int}), we find some additional insights. If we ignore `NoQues', then we again see color detection and computer screen reading problem. However, now we can find another interesting pairs of words `look' and `like'. This pair indicates a subjective question, where the user is asking how the user is looking like. This identifies another challenges of people with visual impairments \textbf{Impression Management}. Another interesting common pair of words are 'long' and 'cook' which indicates reading label issues, however this can be a household activity issue. The trigrams also gave us some new interesting insights (Figure ~\ref{fig:tri_int}). Most of the trigrams confirms above mentioned challenges, however, there are some new issues. One interesting trigram is `display', `treadmill', and `tell' which indicates the health fitness related issues or \textbf{Health Management} Issue. Due to the accessibility issues in health monitoring and fitness monitoring issues, they can not manage health effective. Therefore, the users often seek help for reading the display. Another interesting three words are `pregnancy',`test',`show' which can also be put into health Management category. However, this seems a private information, but still people with visual impairments have to share this information due to their visual challenges.


\subsection{Challenges}
Based on the rigorous analysis, we identified some challenges of people with visual impairments. In this section, we discuss the challenges:

\subsubsection{Object Detection:}


\begin{acks}

The authors would like to thank Professor Gregor von Laszewski for helping us with the instruction and resources that were required to complete this paper. We would also to like to thank the associate instructors for being available on the course website all the time and helping us with their answers.

\end{acks}



\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 
\newpage
\appendix
\section{Appendix}

The bigram and trigram images. 
\begin{figure*}[bp]
        \centering
        %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
        \includegraphics[scale=0.45]{images/bigram_all.png}  
        \caption{Most frequently used pair of words} 
          \label{fig:bi_all}   
\end{figure*}

\begin{figure*}[bp]
        \centering
        %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
        \includegraphics[scale=0.45]{project/images/bigram_interesting.png}  
        \caption{Most frequently used pair of interesting words} 
          \label{fig:bi_int}   
\end{figure*}
\begin{figure*}[bp]
        \centering
        %\includegraphics[width=\textwidth]{figures/appPermissions.pdf}  
        \includegraphics[scale=0.5]{project/images/trigram_interesting.png}  
        \caption{Most frequently used interesting words} 
          \label{fig:tri_int}   
\end{figure*}

%We include an appendix with common issues that we see when students submit papers. One particular important issue is not to use the underscore in bibtex labels. Sharelatex allows this, but the proceedings script we have does not allow this.

%When you submit the paper you need to address each of the items in the
%issues.tex file and verify that you have done them. Please do this
%only at the end once you have finished writing the paper. To d this
%cange TODO with DONE. However if you check something on with DONE, but
%we find you actually have not executed it correcty, you will receive
%point deductions. Thus it is important to do this correctly and not
%just 5 minutes before the deadline. It is better to do a late
%submission than doing the check in haste. 

\input{issues}

\end{document}
